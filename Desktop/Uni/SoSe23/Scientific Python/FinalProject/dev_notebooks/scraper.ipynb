{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "path = os.getcwd()\n",
    "# Specify the category you want to search in\n",
    "# category = \"herren/schuhe/sneaker\"  # replace with the category you want\n",
    "\n",
    "# Define the URL of the site\n",
    "url = f\"https://www.vinted.de/\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Send HTTP request to site and save the response from server in a response object called r\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "# Create a BeautifulSoup object and specify the parser\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Extract the desired info (the offers in this case)\n",
    "# The selector depends on the structure of the webpage, inspect the webpage to find the right selector\n",
    "offers = soup.select('div.offer')\n",
    "\n",
    "\n",
    "# Get response code\n",
    "print(r)\n",
    "\n",
    "# Get HTML text\n",
    "page = requests.get(url)\n",
    "index_page = page.text\n",
    "# print(page.text)\n",
    "with open (\"test.txt\",'w') as f:\n",
    "    f.write((str(page.text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain different category urls for more detailed scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 283 categories for women on vinted that we can scrape\n",
      "We found 190 categories for men vinted that we can scrape\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2854411241.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/zs/bd_b6ytj5l916s0yf_qyvlp80000gq/T/ipykernel_10876/2854411241.py\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    return cat_names_damen, cat_names_herren\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "def find_all(string, substring):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = string.find(substring, start)\n",
    "        if start == -1: return\n",
    "        end = start + 100\n",
    "        yield string[start:end]\n",
    "        start += len(substring) # use start += 1 to find overlapping matches\n",
    "\n",
    "\n",
    "# def \n",
    "\n",
    "substring_damen, substring_herren = \"/damen\", \"/herren\"\n",
    "\n",
    "\n",
    "cat_urls_uncut_damen = set(find_all(page.text, substring_damen))\n",
    "cat_urls_uncut_herren = set(find_all(page.text, substring_herren))\n",
    "\n",
    "\n",
    "\n",
    "cat_urls_cut_damen, cat_urls_cut_herren, cat_names_damen, cat_names_herren = [],[],[],[]\n",
    "\n",
    "\n",
    "for i in cat_urls_uncut_damen:\n",
    "    i = i.split(sep='\\\"')\n",
    "    cat_urls_cut_damen.append(\"https://www.vinted.de{}\".format(i[0]))\n",
    "    cat_names_damen.append(i[0])\n",
    "    # print(i)\n",
    "\n",
    "for i in cat_urls_uncut_herren:\n",
    "    i = i.split(sep='\\\"')\n",
    "    i = \"https://www.vinted.de{}\".format(i[0])\n",
    "\n",
    "print(\"We found {} categories for women on vinted that we can scrape\".format(len(cat_urls_uncut_damen)))\n",
    "print(\"We found {} categories for men vinted that we can scrape\".format(len(cat_urls_uncut_herren)))\n",
    "# for name in cat_names_damen:\n",
    "#     print(name)\n",
    "return cat_names_damen, cat_names_herren\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_urls_cut_damen)\n",
    "print(cat_names_damen)\n",
    "print(len(cat_urls_cut_damen))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get HTMLs of all subcategories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the HTML is not consistent with the use of variable like titles, id etc. we need to obtain the display names of the categories we want to get data from. For that reason we format the names given in the headers of the category pages. this may seem not as straight forward but is the easiest way to actually get the IDs and categories we want to obtain. Call it a workaround :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = path + '/categories'\n",
    "if not os.path.exists(sub_path):\n",
    "    os.makedirs(sub_path)\n",
    "print(sub_path)\n",
    "\n",
    "\n",
    "\n",
    "display_titles = []\n",
    "\n",
    "for url, name in tqdm(zip(cat_urls_cut_damen,cat_names_damen)):\n",
    "    # Send HTTP request to site and save the response from server in a response object called r\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Get the index of display title \n",
    "    index_title = r.text.find(\"<title>\")\n",
    "    title_uncut = r.text[index_title:index_title+100]\n",
    "    splt = title_uncut.split(sep=\" |\")\n",
    "    display_titles.append(splt[0])\n",
    "\n",
    "    # Create a BeautifulSoup object and specify the parser      \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    filename = \"%s.txt\" % name[1:]\n",
    "    filename = filename.replace(\"/\",\"_\")\n",
    "    # print(filename)\n",
    "    # print(type(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_cut = []\n",
    "for title in display_titles:\n",
    "    title = title[7:]\n",
    "    title = title.replace(\"&amp;\",\"\\\\u0026\")\n",
    "    titles_cut.append(title)\n",
    "print(len(titles_cut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"list_names.txt\", 'w') as f:\n",
    "    for i in titles_cut:\n",
    "        f.writelines(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Clutches', 1)\n",
      "('Sonnenbrillen', 1)\n",
      "('Abendkleider', 1)\n",
      "('Regenschirme', 1)\n",
      "('BHs', 2)\n",
      "('Jacken im Military- \\\\u0026 Utility-Stil', 1)\n",
      "('Sneaker', 1)\n",
      "('Sportschuhe für drinnen', 1)\n",
      "('Strumpfhosen', 1)\n",
      "('Einzelne Anzugteile', 1)\n",
      "('Accessoires', 2)\n",
      "('Trainingsanzüge', 1)\n",
      "('Strandblusen \\\\u0026 Sarongs', 1)\n",
      "('Sandalen mit Absatz', 1)\n",
      "('Trapezröcke', 1)\n",
      "('Parfums', 1)\n",
      "('Socken', 1)\n",
      "('Mäntel', 1)\n",
      "('Rucksäcke', 1)\n",
      "('Arbeitsstiefel', 1)\n",
      "('Hosen', 1)\n",
      "('Röhrenjeans', 1)\n",
      "('Motorradstiefel', 1)\n",
      "('Shorts \\\\u0026 Capris', 1)\n",
      "('Übergroße Tücher \\\\u0026 Schals', 1)\n",
      "('Leggings', 1)\n",
      "('Ringe', 1)\n",
      "('Snowboard Boots', 1)\n",
      "('Blusen', 1)\n",
      "('Minikleider', 1)\n",
      "('Jeansjacken', 1)\n",
      "('Bootsschuhe, Loafer \\\\u0026 Mokassins', 1)\n",
      "('Ripped Jeans', 1)\n",
      "('Haarstyling-Zubehör', 1)\n",
      "('Cargoshorts', 1)\n",
      "('Bauchtaschen', 1)\n",
      "('Maxikleider', 1)\n",
      "('Sonstige Accessoires', 1)\n",
      "('Mäntel \\\\u0026 Jacken', 1)\n",
      "('Tops \\\\u0026 T-Shirts', 1)\n",
      "('Unterwäsche', 1)\n",
      "('Beutel', 1)\n",
      "('Oberteile \\\\u0026 T-Shirts', 1)\n",
      "('Sportkleidung', 1)\n",
      "('Skater-Röcke', 1)\n",
      "('Schlittschuhe', 1)\n",
      "('Bauchfreie Tops', 1)\n",
      "('Schwangerschafts- \\\\u0026 Still-BHs', 1)\n",
      "('Jeans', 1)\n",
      "('Slips', 1)\n",
      "('Pumps', 1)\n",
      "('Shorts mit tiefer Taille', 1)\n",
      "('Bleistiftröcke', 1)\n",
      "('Klassische \\\\u0026 lange Mäntel', 1)\n",
      "('Fleecejacken', 1)\n",
      "('Hoodies \\\\u0026 Langarmshirts', 2)\n",
      "('Lederhosen', 1)\n",
      "('Badelatschen', 1)\n",
      "('Uhren', 1)\n",
      "('Folkloretaschen', 1)\n",
      "('Shorts', 2)\n",
      "('Baretts', 1)\n",
      "('Handpflege', 1)\n",
      "('Mary Janes', 1)\n",
      "('Hosen \\\\u0026 Leggins', 1)\n",
      "('Anderer Schmuck', 1)\n",
      "('Tuniken', 1)\n",
      "('Sommerkleider', 1)\n",
      "('Schützer', 1)\n",
      "('Röcke mit hoher Taille', 1)\n",
      "('Ketten \\\\u0026 Anhänger', 1)\n",
      "('Jeansröcke', 1)\n",
      "('Portemonnaies', 1)\n",
      "('Hausschuhe, Pantoffeln \\\\u0026 Slipper', 1)\n",
      "('Bikinis', 1)\n",
      "('Capri-Hosen', 1)\n",
      "('Midiröcke', 1)\n",
      "('Fahrradschuhe', 1)\n",
      "('Winterkleider', 1)\n",
      "('Beauty-Zubehör', 1)\n",
      "('Schlaghosen', 1)\n",
      "('Schals', 2)\n",
      "('Midikleider', 1)\n",
      "('Cabans \\\\u0026 Kurzmäntel', 1)\n",
      "('Ohrringe', 1)\n",
      "('Overknees', 1)\n",
      "('Teddymäntel', 1)\n",
      "('Andere Jump- \\\\u0026 Playsuits', 1)\n",
      "('Fußballschuhe', 1)\n",
      "('Schlüsselanhänger', 1)\n",
      "('Boleros', 1)\n",
      "('Shirts, Tops \\\\u0026 Blusen', 1)\n",
      "('¾-ärmelige Pullover', 1)\n",
      "('Badeanzüge', 1)\n",
      "('Andere Accessoires', 1)\n",
      "('Sandalen, Pantoletten \\\\u0026 Badelatschen', 1)\n",
      "('Unterwäsche \\\\u0026 Nachtwäsche', 1)\n",
      "('Schwimm- \\\\u0026 Wasserschuhe', 1)\n",
      "('Kleidung', 1)\n",
      "('Schmuck', 1)\n",
      "('Rollschuhe \\\\u0026 Inlineskates', 1)\n",
      "('Rollkragen', 1)\n",
      "('Hosen nach Maß', 1)\n",
      "('Trenchcoats', 1)\n",
      "('Kleine Schwarze', 1)\n",
      "('Technische Accessoires', 1)\n",
      "('Figur formende Unterwäsche', 1)\n",
      "('Reisetaschen', 1)\n",
      "('Tulpenröcke', 1)\n",
      "('Windbreaker', 1)\n",
      "('Pullover', 1)\n",
      "('Faltenröcke', 1)\n",
      "('Einteiler', 1)\n",
      "('Kurzärmelige Blusen', 1)\n",
      "('Damen - Vinted</title>\\n  <meta name=\"description\" content=\"Bring frischen Wind in deine Damen', 1)\n",
      "('Warme Mützen', 1)\n",
      "('Formelle \\\\u0026 Business-Kleider', 1)\n",
      "('Schnürshorts', 1)\n",
      "('Shorts mit hoher Taille', 1)\n",
      "('Basketballschuhe', 1)\n",
      "('Sportschuhe', 1)\n",
      "('Sonstiges', 15)\n",
      "('Beauty', 1)\n",
      "('Flache Sandalen', 1)\n",
      "('Schulterfreie Tops', 1)\n",
      "('Absatzschuhe', 1)\n",
      "('Andere Pullover', 1)\n",
      "('Espadrilles', 1)\n",
      "('Make-up', 1)\n",
      "('Gerade geschnittene Jeans', 1)\n",
      "('Umstands-Nachtwäsche', 1)\n",
      "('Overalls für Schwangere', 1)\n",
      "('Röhrenhosen', 1)\n",
      "('Jacken \\\\u0026 Mäntel', 1)\n",
      "('Körperpflege', 1)\n",
      "('Dreiviertel-Hosen \\\\u0026 Chinos', 1)\n",
      "('Laufschuhe', 1)\n",
      "('Haarpflege', 1)\n",
      "('Ledershorts', 1)\n",
      "('Kosmetiktaschen', 1)\n",
      "('Party \\\\u0026 Cocktail', 1)\n",
      "('Hosenanzüge', 1)\n",
      "('Nachtwäsche \\\\u0026 Pyjamas', 1)\n",
      "('Tanzschuhe', 1)\n",
      "('Kletterschuhe', 1)\n",
      "('Bomberjacken', 1)\n",
      "('Jeansshorts', 1)\n",
      "('Lange Pullover', 1)\n",
      "('Flache Schuhe', 1)\n",
      "('Lässige Kleider', 1)\n",
      "('Strickpullover', 1)\n",
      "('Schneestiefel', 1)\n",
      "('Blazer', 1)\n",
      "('Rückenfreie Kleider', 1)\n",
      "('Jeans mit hoher Taille', 1)\n",
      "('Schwangerschafts- \\\\u0026 Postpartum-Gürtel', 1)\n",
      "('Brillen', 1)\n",
      "('Kopftücher', 1)\n",
      "('Skistiefel', 1)\n",
      "('Nagelpflege \\\\u0026 -zubehör', 1)\n",
      "('Gürtel', 1)\n",
      "('Schnürschuhe', 1)\n",
      "('Stiefeletten', 1)\n",
      "('Hüte \\\\u0026 Mützen', 1)\n",
      "('Capes \\\\u0026 Ponchos', 1)\n",
      "('Schößchentops', 1)\n",
      "('Langärmelige Blusen', 1)\n",
      "('Jumpsuits \\\\u0026 Playsuits', 1)\n",
      "('Ballerinas', 1)\n",
      "('Kimonos', 1)\n",
      "('Shirts', 1)\n",
      "('Miniröcke', 1)\n",
      "('Tragetaschen', 1)\n",
      "('Mützen', 1)\n",
      "('Schlauchröcke', 1)\n",
      "('Hosen mit weitem Bein', 1)\n",
      "('Für besondere Anlässe', 1)\n",
      "('Zehentrenner', 1)\n",
      "('Bademode', 1)\n",
      "('Sets', 1)\n",
      "('Westen', 3)\n",
      "('Hosen \\\\u0026 Leggings', 1)\n",
      "('Hemdjacken', 1)\n",
      "('Kostüme und Abendanzüge', 1)\n",
      "('Jeanskleider', 1)\n",
      "('Oberbekleidung', 1)\n",
      "('Badekleidung', 1)\n",
      "('Pullover mit V-Ausschnitt', 1)\n",
      "('Cropped Jeans', 1)\n",
      "('Biker- \\\\u0026 Racer-Jacken', 1)\n",
      "('Clogs', 1)\n",
      "('Regenmäntel', 1)\n",
      "('Parkas', 1)\n",
      "('Activewear', 1)\n",
      "('Gesichtspflege', 1)\n",
      "('Nagelpflege', 1)\n",
      "('Röcke', 3)\n",
      "('Gerade geschnittene Hosen', 1)\n",
      "('Fußkettchen', 1)\n",
      "('Neckholder-Tops', 1)\n",
      "('Haarschmuck', 1)\n",
      "('Boyfriend Jeans', 1)\n",
      "('Körperpflege-Zubehör', 1)\n",
      "('Stiefel', 1)\n",
      "('Trägerlose Kleider', 1)\n",
      "('Gesichtspflege-Zubehör', 1)\n",
      "('Uhren \\\\u0026 Armbänder', 1)\n",
      "('Jacken', 1)\n",
      "('Andere Anzüge \\\\u0026 Blazer', 1)\n",
      "('Knielange Shorts', 1)\n",
      "('Accessoires für Dessous', 1)\n",
      "('Wander- \\\\u0026 Trekkingschuhe', 1)\n",
      "('¾-Blusen', 1)\n",
      "('Tennisschuhe', 1)\n",
      "('Gummistiefel', 1)\n",
      "('Collegejacken', 1)\n",
      "('Hoodies \\\\u0026 Pullover', 1)\n",
      "('Dufflecoats', 1)\n",
      "('T-Shirts', 1)\n",
      "('Hüte', 2)\n",
      "('Halbhohe Stiefel', 1)\n",
      "('Maxi-Röcke', 1)\n",
      "('Blazer \\\\u0026 Anzüge', 1)\n",
      "('Abschlussballkleider', 1)\n",
      "('Bademäntel', 1)\n",
      "('Hochzeitskleider', 1)\n",
      "('Umstandsunterwäsche', 1)\n",
      "('Strickschals', 1)\n",
      "('Hallenfußballschuhe', 1)\n",
      "('Golfschuhe', 1)\n",
      "('Umhängetaschen', 1)\n",
      "('Broschen', 1)\n",
      "('Tücher \\\\u0026 Schals', 1)\n",
      "('Taschen', 1)\n",
      "('Stirnbänder', 1)\n",
      "('Steppjacken', 1)\n",
      "('Weekender', 1)\n",
      "('Make-up-Zubehör', 1)\n",
      "('Strickjacken', 1)\n",
      "('Handtaschen', 1)\n",
      "('Bodys', 1)\n",
      "('Kostüme \\\\u0026 Besonderes', 1)\n",
      "('Umstandskleidung', 1)\n",
      "('Kleider', 3)\n",
      "('Kniehohe Stiefel', 1)\n",
      "('Trägerhemden', 1)\n",
      "('Pluderhosen', 1)\n",
      "('Schuhe', 1)\n",
      "('Pullover und Jacken', 1)\n",
      "('Overalls', 1)\n",
      "('Ski- \\\\u0026 Snowboard-Jacken', 1)\n",
      "('Daunenjacken', 1)\n",
      "('Armbänder', 1)\n",
      "('Schmucksets', 1)\n",
      "('Handschuhe', 2)\n",
      "('Rollkragenpullover', 1)\n"
     ]
    }
   ],
   "source": [
    "doubles = []\n",
    "for i in titles_cut:\n",
    "    c = titles_cut.count(i)\n",
    "\n",
    "        # print(\"Title: {} occured {} times.\".format(i,c))\n",
    "    doubles.append((i,c))\n",
    "\n",
    "set_d = set(doubles)\n",
    "for i in set_d:\n",
    "    print(i)\n",
    "names = list(set_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the overview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "     id          title              code item_count  \\\n",
      "0   159       Clutches          CLUTCHES     827256   \n",
      "0    26  Sonnenbrillen        SUNGLASSES     697604   \n",
      "0  1778   Abendkleider  EVENING_DRESSES1    2000770   \n",
      "0  1851   Regenschirme   W_ACC_UMBRELLAS      11280   \n",
      "0   119            BHs              BRAS    1187570   \n",
      "\n",
      "                                                url  \n",
      "0                           /damen/taschen/clutches  \n",
      "0                  /damen/accessoires/sonnenbrillen  \n",
      "0  /damen/kleidung/kleider/fur-anlasse/abendkleider  \n",
      "0                   /damen/accessoires/regenschirme  \n",
      "0   /damen/kleidung/unterwasche-and-nachtwasche/bhs  \n"
     ]
    }
   ],
   "source": [
    "# names = set(titles_cut)\n",
    "properties = ['id','title','code','item_count','url']\n",
    "\n",
    "\n",
    "def get_single_property(str:str,p:str):\n",
    "    i = str.find(p)\n",
    "    sub = str[i:]\n",
    "    sub = sub.split(\",\")\n",
    "    sub = sub[0]\n",
    "    prop_len = len(p)\n",
    "    name = sub[:prop_len]\n",
    "    sub = sub[prop_len+1:]\n",
    "    return name, sub\n",
    "\n",
    "\n",
    "\n",
    "def get_dataframe(names):\n",
    "\n",
    "    for i in names:\n",
    "        start = 0\n",
    "        for j in range(i[1]):\n",
    "            \n",
    "            list_name = []\n",
    "            dict_props = {}\n",
    "            ind = index_page.find(i[0], start)\n",
    "            \n",
    "            # Get rough length to be cut \n",
    "            sub = index_page[ind-20:ind+1000]\n",
    "            \n",
    "            # Assign the new start for str.find() as to not retrieve the same index 2x\n",
    "            start = ind + len(sub)\n",
    "\n",
    "\n",
    "            sub = sub.replace(\"{\",\"\")\n",
    "            sub = sub.replace(\"\\\"\",\"\")\n",
    "\n",
    "            # Obtain all single desired properties of substring\n",
    "            for p in properties:\n",
    "                name, value = get_single_property(sub, p)\n",
    "                dict_props[p] = value\n",
    "\n",
    "\n",
    "            # convert the dictionary to a pandas DataFrame\n",
    "            df = pd.DataFrame(dict_props, index=[0])\n",
    "            yield df\n",
    "\n",
    "\n",
    "dfs = list(get_dataframe(names))\n",
    "combined = pd.concat(dfs)\n",
    "\n",
    "# Check that there is no duplicates in list and have a look into the df\n",
    "print( True in combined['id'].duplicated())\n",
    "print(combined.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category specific retrieving of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax of the urls to retrieve from  : https://www.vinted.de/catalog?catalog[]=1070&time=1690115802&page=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_from_url(url):\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    # Send HTTP request to site and save the response from server in a response object called r\n",
    "    r = requests.get(catalog_url, headers=headers)\n",
    "\n",
    "    # Create a BeautifulSoup object and specify the parser\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Extract the desired info (the offers in this case)\n",
    "    # The selector depends on the structure of the webpage, inspect the webpage to find the right selector\n",
    "    offers = soup.select('div.offer')\n",
    "\n",
    "\n",
    "    # Get HTML text\n",
    "    page = requests.get(catalog_url)\n",
    "    return page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_ids(page):\n",
    "    length = 1500\n",
    "    s = \"{\\\"catalogItems\\\":{\\\"ids\\\":\"\n",
    "    index = page.find(s)\n",
    "    sub = page[index:index+length]\n",
    "\n",
    "    # Remove everything around the ids\n",
    "    splts = sub.split(\"[\")\n",
    "    sub = splts[1]\n",
    "    splts = sub.split(\"]\")\n",
    "    sub = splts[0]\n",
    "    \n",
    "    splt = sub.split(\",\")\n",
    "    for i in splt:\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_category(html,subs):\n",
    "\n",
    "    for i in subs:\n",
    "        dict_props = {}\n",
    "        ind = html.find(i)\n",
    "        \n",
    "        # Get rough length to be cut \n",
    "        sub = html[ind:ind+5000]\n",
    "        splt = sub.split(\"\\\",\\\"content_source\\\"\")\n",
    "        sub = splt[0]    \n",
    "\n",
    "\n",
    "        sub = sub.replace(\"{\",\"\")\n",
    "        sub = sub.replace(\"\\\"\",\"\")\n",
    "\n",
    "        # Obtain all single desired properties of substring\n",
    "        for p in properties_announce:\n",
    "            name, value = get_single_property(sub, p)\n",
    "            dict_props[p] = value\n",
    "\n",
    "\n",
    "        # convert the dictionary to a pandas DataFrame\n",
    "        df = pd.DataFrame(dict_props, index=[0])\n",
    "        # print(df.head())\n",
    "\n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0856529236f8462baf77689e00e64c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import time\n",
    "catalog_url = \"https://www.vinted.de/catalog?catalog[]=1070&page=3\"\n",
    "\n",
    "\n",
    "\n",
    "def get_announces_by_category(category_id, nr_pages):\n",
    "\n",
    "    properties_announce = ['id','title','price','brand_title','size_title','user:id','login','profile_url'] \n",
    "\n",
    "    for page in tqdm(range(nr_pages_to_scrape)):\n",
    "        try:\n",
    "                \n",
    "            url = \"https://www.vinted.de/catalog?catalog[]={}&page{}\".format(category_id,page+1)\n",
    "            # print(url)\n",
    "            html_string = get_html_from_url(url)\n",
    "            # print(html_string)\n",
    "\n",
    "            l = list(get_items_ids(html_string))\n",
    "            unique_id_references = [\"\\\"id\\\":\"+i for i in l]\n",
    "            # print(unique_id_references)\n",
    "\n",
    "            announces = list(get_dataframe_category(html=html_string, subs=unique_id_references))\n",
    "\n",
    "            announces_df = pd.concat(announces)\n",
    "            # print(announces_df.head())\n",
    "            time.sleep(1)\n",
    "            yield announces_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "\n",
    "\n",
    "category_id = 1070\n",
    "nr_pages_to_scrape = 10\n",
    "list_category_announces = list(get_announces_by_category(category_id,nr_pages_to_scrape))\n",
    "df_category_announces = pd.concat(list_category_announces)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950\n",
      "           id                title price brand_title   size_title    user:id  \\\n",
      "0  3220117943         Green pants   10.0      Promod  XS / 34 / 6  144045060   \n",
      "0  2011568095       Hose H\\u0026M    5.0    H\\u0026M  M / 38 / 10   63212719   \n",
      "0  3132782382     ZARA Cargo Pants   5.0        Zara  XS / 34 / 6  148962655   \n",
      "0  2791054029            Trousers    4.0    H\\u0026M  M / 38 / 10  135202222   \n",
      "0  2982735301  Schwarze Stoff Hose   5.0  Zara Basic   S / 36 / 8  108129047   \n",
      "\n",
      "          login                                        profile_url  \n",
      "0     ritta9944   https://www.vinted.de/member/144045060-ritta9944  \n",
      "0     ilarialom    https://www.vinted.de/member/63212719-ilarialom  \n",
      "0  sophia_eli_b  https://www.vinted.de/member/148962655-sophiaelib  \n",
      "0      swalter2    https://www.vinted.de/member/135202222-swalter2  \n",
      "0       julyhel     https://www.vinted.de/member/108129047-julyhel  \n"
     ]
    }
   ],
   "source": [
    "print(df_category_announces.shape[0])\n",
    "print(df_category_announces.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
